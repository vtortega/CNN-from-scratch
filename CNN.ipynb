{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "442347bf-41b4-4035-85a2-37463559bee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitor/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import glob\n",
    "from pandas.core.common import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8867494-d3f6-4199-b508-89644d42abc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vitor/Downloads/PetImages/training_data/Dog/7472.jpg\n",
      "Train size:  24190\n",
      "Test size:  812\n"
     ]
    }
   ],
   "source": [
    "# Creating a custom Dataset\n",
    "\n",
    "train_image_paths = []\n",
    "classes = []\n",
    "\n",
    "for data_path in glob.glob(training_data_path + '/*'):\n",
    "    classes.append(data_path.split('/')[-1]) \n",
    "    train_image_paths.append(glob.glob(data_path + '/*'))\n",
    "    # Take the last dir as a class, Cat or Dog, and add to classes.\n",
    "    # Take the paths for each image to train_image_paths.\n",
    "\n",
    "train_image_paths = list(flatten(train_image_paths))\n",
    "\n",
    "print(train_image_paths[5])\n",
    "# Transforms the list of lists(2 in this case) in to a list.\n",
    "\n",
    "# Same thing with test data.\n",
    "test_image_paths = []\n",
    "\n",
    "for data_path in glob.glob(test_data_path + '/*'):\n",
    "    test_image_paths.append(glob.glob(data_path + '/*'))\n",
    "    \n",
    "test_image_paths = list(flatten(test_image_paths))\n",
    "\n",
    "print(\"Train size: \", len(train_image_paths))\n",
    "print(\"Test size: \", len(test_image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37554d20-079b-4bc8-96ee-15aa3d5e6e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Dog', 1: 'Cat'}\n",
      "{'Dog': 0, 'Cat': 1}\n"
     ]
    }
   ],
   "source": [
    "# Index each class\n",
    "\n",
    "idx_to_class = {i:j for i, j in enumerate(classes)}\n",
    "print(idx_to_class)\n",
    "\n",
    "class_to_idx = {value:key for key,value in idx_to_class.items()}\n",
    "print(class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ba5f661-529d-4fe8-8d7e-c30cf45a6e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatOrDogImageDataset(Dataset):\n",
    "    def __init__(self, train_image_paths, classes, transform=None, target_transform=None):\n",
    "        self.img_dir = train_image_paths\n",
    "        self.img_labels = classes\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = train_image_paths[idx]\n",
    "        image = read_image(img_path)\n",
    "        # Converts to Tensor.\n",
    "        label = self.img_labels[idx]\n",
    "        label = class_to_idx.get(label, None)\n",
    "        #Get the path and label of one data point.\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        #Make any necessary transformation.\n",
    "\n",
    "        return image, label\n",
    "        # Return the image and the respective label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f5a1d50-6ab5-4a26-a70a-6f5606850217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, CenterCrop, ToTensor, Lambda, Normalize, Resize\n",
    "\n",
    "img_transform = Compose(\n",
    "    [Resize(400),\n",
    "    CenterCrop(400),])\n",
    "# Since the images do not have the same dimensions, we need to do something. I chose cropping.\n",
    "# ToTensor transforms the image into a FloatTensor and scales the pixel values.\n",
    "\n",
    "target_transform = Lambda(lambda y: torch.zeros(\n",
    "    2, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
    "# Apply a desired lambda function defined by us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e41e868-9843-4e1e-8cd8-d0f802f072bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geting the Datasets\n",
    "\n",
    "training_data = CatOrDogImageDataset(train_image_paths, classes, img_transform, target_transform)\n",
    "testing_data = CatOrDogImageDataset(test_image_paths, classes, img_transform, target_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c11a7b1-b727-45ed-9ce4-a780a135f648",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=4, shuffle=True)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4521f7a-d602-42b4-bb82-5f3d4f821438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bd88d40-2687-49c7-856d-1e26e59d0e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c62f8b52-f15c-4de2-9978-5ea594556892",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(150544, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61543619-03ab-46c6-b83b-aae50fa8fc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "072e26ba-4ca7-4db2-a443-28d11d9a842f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # Get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs = inputs/255\n",
    "\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 19:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9870d3df-cabe-4789-b527-eb023143b01e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
